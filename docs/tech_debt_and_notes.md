# Technical Debt & Notes

Below are items of current technical debt and known limitations in our RAG platform.

---

## 1. Retrieval Module Scope

- **Current Support**: Only plain‐text PDF files are handled.  
- **Limitation**: If a PDF contains images, charts, scanned pages, or non‐textual elements, those parts are ignored.  
- **Future Consideration**: We may need to integrate OCR (e.g., Tesseract) or table‐extraction libraries to capture text from images or complex layouts.

---

## 2. Chunking Strategy

- **Current Approach**: We split by character count (e.g., 500 characters per chunk) while ensuring we do not break sentences in half (via NLTK’s `sent_tokenize`).  
- **Limitation**:  
  - This method does not account for semantic boundaries (e.g., short paragraphs or headings may be split arbitrarily).  
  - Multi‐paragraph contexts could be split across chunks in ways that dilute meaning.  
- **Future Improvement**:  
  - Use a token‐based approach (e.g., Hugging Face tokenizer) to respect the LLM’s actual token limits.  
  - Consider paragraph‐level or section‐level chunking so that coherent blocks of text remain intact.

---

## 3. LLM Model Limitations

- **Current Model**: `google/flan-t5-small` (runs locally, CPU‐only).  
- **Key Constraints**:  
  1. **Model Size**: flan‐t5‐small has a small parameter count (~80M parameters), limiting both comprehension and generation quality.  
  2. **Token Window**: The maximum input + output token length is limited (≈512 tokens total). When the concatenated “context” is large, much of it may be truncated, resulting in overly brief—or even “just the title”—answers.  
  3. **Generation Quality**: For complex legal questions, flan‐t5‐small often produces generic or incomplete responses.  
- **Future Plans**:  
  - Switch to a larger local model (e.g., `flan-t5-large` or `llama-2-13b`) or integrate a cloud‐based LLM (OpenAI GPT‐4).  
  - Dynamically truncate or summarize retrieved context if it exceeds the model’s token budget.

---

## 4. Frontend Status

- **Current UI**: Only Swagger UI (auto‐generated by FastAPI) is available for testing endpoints.  
- **Limitation**:  
  - No custom React or Vue frontend to guide users through PDF upload, question entry, or agent orchestration flows.  
  - Lacks a user‐friendly interface to display retrieved context snippets or format generated answers.  
- **Next Steps**:  
  - Develop a simple React frontend that:  
    1. Allows drag‐and‐drop or file‐picker for PDF upload.  
    2. Provides a form for entering questions and shows “retrieved context” alongside the LLM’s answer.  
    3. Later, add a “Modular Agent Designer” UI for assembling multiple modules in a flow.

---
